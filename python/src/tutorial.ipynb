{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial for evaluating the experiments\n",
    "\n",
    "In total, we provide three pre-trained models for the 3-SAT dataset and six pre-trained models for the pseudo-industrial datasets. They found in the folders `../../Data/models/random_3SAT/` and `../../Data/models/pseudo-industrial/`, respectively. The following table summarizes the characteristics of the models.\n",
    "\n",
    "| Path of the pre-trained model        | Description of model                                                                                                    |\n",
    "|----------------------------------|-----------------------------------------------------------------------------------------------------------------|\n",
    "| `../../Data/models/random_3SAT/3_SAT_LLL.npy`| model trained on random 3-SAT using only the LLL-Loss     |\n",
    "| `../../Data/models/random_3SAT/3_SAT_Gibbs.npy`|  model trained on random 3-SAT using only the Gibbs-Loss |\n",
    "| `../../Data/models/random_3SAT/3_SAT_Gibbs_LLL.npy`|model trained on random 3-SAT using both the LLL-Loss and the Gibbs-Loss |\n",
    "| `../../Data/models/pseudo_industrial/g4sat_easy_ca.npy`| model trained on easy CA instances using only the Gibbs-Loss     |\n",
    "| `../../Data/models/pseudo_industrial/g4sat_medium_ca.npy`| model trained on medium CA instances using only the Gibbs-Loss     |\n",
    "| `../../Data/models/pseudo_industrial/g4sat_hard_ca.npy`| model trained on hard CA instances using only the Gibbs-Loss     |\n",
    "| `../../Data/models/pseudo_industrial/g4sat_easy_ps.npy`| model trained on easy PS instances using only the Gibbs-Loss     |\n",
    "| `../../Data/models/pseudo_industrial/g4sat_medium_ps.npy`| model trained on medium PS instances using only the Gibbs-Loss     |\n",
    "| `../../Data/models/pseudo_industrial/g4sat_hard_ps.npy`| model trained on hard PS instances using only the Gibbs-Loss     |\n",
    "\n",
    "The corresponding hyperparameters used for the training are specified in the corresponding config files in `../../experiments/configs/`.\n",
    "\n",
    "## 1) Experiments for random 3-SAT\n",
    "\n",
    "### 1.1) Load a model and run the MT algorithm and WalkSAT on it\n",
    "\n",
    "Let us start by the full-oracle version that uses the oracle both for initialization and for updating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluate_with_given_params import load_model_and_test\n",
    "\n",
    "data_path = r\"../../Data/random_sat_data/test/\" #tbf with path of the evaluation dataset\n",
    "!mkdir ../../Data/trajectories/ #create a folder to save the trajectories\n",
    "!mkdir ../../Data/trajectories/random_3SAT/ #create a folder to save the trajectories\n",
    "\n",
    "trajectories_path = r\"../../Data/trajectories/random_3SAT/\"\n",
    "\n",
    "#put the paths of the models you want to evaluate\n",
    "model_paths = [ # \"uniform\" #for uniform algorithm,\n",
    "                # or path of the models you want to evaluate\n",
    "                # we used the following:\n",
    "                \"uniform\",\n",
    "                \"../../Data/models/random_3SAT/3_SAT_LLL.npy\",\n",
    "                \"../../Data/models/random_3SAT/3_SAT_Gibbs_LLL.npy\", \n",
    "                \"../../Data/models/random_3SAT/3_SAT_Gibbs.npy\",\n",
    "                ]\n",
    "# you can put multiple elements in this list! \n",
    "# in case you train your own models, please adjust the filename accordingly.\n",
    "\n",
    "n_steps = 1000000 \n",
    "n_runs = 100\n",
    "\n",
    "# We start by running the evaluation using the MT algorithm\n",
    "\n",
    "for model_path in model_paths:\n",
    "    if model_path != \"uniform\":\n",
    "        path_save = trajectories_path + \"3SATmoser\" + model_path.split(\"/\")[-1][:-4]\n",
    "    else:\n",
    "        path_save = trajectories_path + \"3SATmoser\" + model_path\n",
    "    total_array2 = load_model_and_test(\n",
    "                    data_path,\n",
    "                    model_path,\n",
    "                    n_steps,\n",
    "                    n_runs,\n",
    "                    \"moser\", #moser for MT-algorithm or probsat for oracle WalkSAT\n",
    "                    path_save=path_save,\n",
    "                    keep_traj=True,\n",
    "                    pre_compute_mapping=True,\n",
    "                    prob_flip_best=0,\n",
    "                )\n",
    "\n",
    "# and then for the oracle WalkSAT algorithm\n",
    "\n",
    "for model_path in model_paths:\n",
    "    if model_path != \"uniform\":\n",
    "        path_save = trajectories_path + \"3SATprobsat\" + model_path.split(\"/\")[-1][:-4]\n",
    "    else:\n",
    "        path_save = trajectories_path + \"3SATprobsat\" + model_path\n",
    "    total_array2 = load_model_and_test(\n",
    "                    data_path,\n",
    "                    model_path,\n",
    "                    n_steps,\n",
    "                    n_runs,\n",
    "                    \"probsat\", #moser for MT-algorithm or probsat for oracle WalkSAT\n",
    "                    path_save=path_save,\n",
    "                    keep_traj=True,\n",
    "                    pre_compute_mapping=True,\n",
    "                    prob_flip_best=0,\n",
    "                )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also only use the oracle for initialization. We do this by the following lines of code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from evaluate_with_given_params import load_model_and_test_two_models\n",
    "\n",
    "data_path = r\"../../Data/random_sat_data/test/\" #tbf with path of the evaluation dataset\n",
    "!mkdir ../../Data/trajectories/random_3SAT/ #create a folder to save the trajectories\n",
    "trajectories_path = r\"../../Data/trajectories/random_3SAT/\"\n",
    "\n",
    "#put the paths of the models you want to evaluate\n",
    "model_path_resample = \"uniform\"\n",
    "model_path_initialize = \"../../Data/models/random_3SAT/3_SAT_Gibbs_LLL.npy\"\n",
    "\n",
    "# in case you train your own models, please adjust the filename accordingly.\n",
    "\n",
    "n_steps = 1000000\n",
    "n_runs = 100\n",
    "if model_path_initialize != \"uniform\":\n",
    "        path_save = trajectories_path + \"3SATmoser_initialize_only\" + model_path_initialize.split(\"/\")[-1][:-4]\n",
    "else:\n",
    "        path_save = trajectories_path + \"3SATmoser_initialize_only\" + model_path_initialize\n",
    "total_array2 = load_model_and_test_two_models(\n",
    "                        data_path,\n",
    "                        model_path_initialize,\n",
    "                        model_path_resample,\n",
    "                        n_steps,\n",
    "                        n_runs,\n",
    "                        \"moser\", #moser for MT-algorithm or probsat for oracle WalkSAT\n",
    "                        path_save=path_save,\n",
    "                        keep_traj=True,\n",
    "                        pre_compute_mapping=True,\n",
    "                        prob_flip_best=0,\n",
    "                    )\n",
    "\n",
    "\n",
    "if model_path_initialize != \"uniform\":\n",
    "        path_save = trajectories_path + \"3SATprobsat_initialize_only\" + model_path_initialize.split(\"/\")[-1][:-4]\n",
    "else:\n",
    "        path_save = trajectories_path + \"3SATprobsat_initialize_only\" + model_path_initialize\n",
    "total_array2 = load_model_and_test_two_models(\n",
    "                        data_path,\n",
    "                        model_path_initialize,\n",
    "                        model_path_resample,\n",
    "                        n_steps,\n",
    "                        n_runs,\n",
    "                        \"probsat\", #moser for MT-algorithm or probsat for oracle WalkSAT\n",
    "                        path_save=path_save,\n",
    "                        keep_traj=True,\n",
    "                        pre_compute_mapping=True,\n",
    "                        prob_flip_best=0,\n",
    "                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2) Plot the trajectories from the evaluation above\n",
    "\n",
    "#### 1.2.1) Define helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.lines as mlines\n",
    "import pandas as pd\n",
    "import os\n",
    "# Set global plot parameters\n",
    "plt.rcParams['axes.labelsize'] = 18\n",
    "plt.rcParams['axes.titlesize'] = 18\n",
    "plt.rcParams['legend.fontsize'] = 12\n",
    "plt.rcParams['text.usetex'] = True\n",
    "plt.rcParams[\"font.family\"] = \"Times New Roman\"\n",
    "\n",
    "def do_evaluation_multiple_models_NEURIPS_legend(saved_eval_path_list_multiple, color, legend, average_alpha = False, plot_save = False, min_max = False):\n",
    "    \"\"\"\n",
    "    saved_eval_path_list_multiple: list of lists of paths to the saved evaluation files (i.e. trajectories.npy files from above) with the first list being for MT and the second for WalkSAT\n",
    "    color: list of colors for the models (i.e. the colors of the models in the same order as the saved_eval_path_list that appear in the figure)\n",
    "    average_alpha: if False, the each instance is plotted in a scatter plot. Otherwise, the median accross instances and runs.\n",
    "    legend: list of names of the models (i.e. the names of the models in the same order as the saved_eval_path_list that appear in the figure legend)\n",
    "    plot_save: if False, the plot is not saved. Otherwise, the path to save the plot.\n",
    "    min_max: if False, only the median is plotted. Otherwise, the min and max are also plotted.\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(2, 2, figsize=(7, 6))\n",
    "    def plot_number_of_steps(idx, total_steps, alpha_array, color, average_alpha = False, legend = False):\n",
    "        label = legend\n",
    "        if average_alpha == False:\n",
    "            ax[idx, 0].scatter([alpha_array],[total_steps], alpha = 0.8, label = label) #model_path.split(\"/\")[-1])\n",
    "        else:\n",
    "            x, x_vary = average_alpha\n",
    "            y = []\n",
    "            for i in x:\n",
    "                relevant_mask = np.where(abs(i-alpha_array) < x_vary, 1, 0)\n",
    "                relevant_steps = total_steps*relevant_mask\n",
    "                relevant_steps = relevant_steps[relevant_steps>0]\n",
    "                relevant_alpha = alpha_array*relevant_mask\n",
    "                relevant_alpha = relevant_alpha[relevant_alpha>0]\n",
    "                y.append(np.median(relevant_steps))\n",
    "            ax[idx, 0].plot(x,y, \".-\", alpha = 0.8, color = color, label = label)\n",
    "\n",
    "    def plot_trajectory_solved_instances(idx, total_steps, energies_array, color, legend = False):\n",
    "        min_steps = np.amin(total_steps, axis = 1)\n",
    "        median_steps = np.median(total_steps, axis = 1)\n",
    "        max_steps = np.amax(total_steps, axis = 1)\n",
    "        lines = []\n",
    "        max_plot_steps = len(energies_array) - 1\n",
    "        for steps, linewidth, alpha, linestyle in zip([min_steps, median_steps, max_steps], \n",
    "                                                    [1, 1.5,1], \n",
    "                                                    [1, 1, 1],\n",
    "                                                    ['dotted', 'solid', 'dashed']):\n",
    "            steps = np.array(steps, dtype=int)\n",
    "            bins = np.logspace(0, np.log10(max_plot_steps), num=10000)\n",
    "            counts, bins = np.histogram(steps, bins=bins)\n",
    "            cumulative_counts = np.cumsum(counts)\n",
    "            percentages = cumulative_counts / len(steps) * 100\n",
    "            if linestyle == 'solid':\n",
    "                line, = ax[idx, 1].plot(bins[1:], percentages, color=color, linewidth=linewidth, alpha=alpha, linestyle=linestyle, label=legend)\n",
    "                lines.append(line)\n",
    "            else:\n",
    "                if min_max == True:\n",
    "                    line, = ax[idx, 1].plot(bins[1:], percentages, color=color, linewidth=linewidth, alpha=alpha, linestyle=linestyle)\n",
    "                    lines.append(line)\n",
    "\n",
    "\n",
    "    for idx, saved_eval_path_list in enumerate(saved_eval_path_list_multiple):\n",
    "        for i in range(len(saved_eval_path_list)):\n",
    "            saved_eval_path = saved_eval_path_list[i]\n",
    "            c = color[i]\n",
    "            l = legend[i]\n",
    "            _ ,_ , _, alpha_array, _, energies_array_median,total_steps = np.load(saved_eval_path, allow_pickle=True)\n",
    "            total_steps = total_steps + np.ones(np.shape(total_steps))\n",
    "            total_steps_median = np.median(total_steps, axis = 1)\n",
    "            plot_number_of_steps(idx, total_steps_median, alpha_array, c, average_alpha=average_alpha, legend=l)\n",
    "            if len(energies_array_median)!= 0:\n",
    "                plot_trajectory_solved_instances(idx, total_steps, energies_array_median, c, legend = l)\n",
    "            if len(saved_eval_path_list) -1 == i:\n",
    "                max_steps = len(energies_array_median)\n",
    "        ax[idx, 0].hlines(max_steps, np.min(alpha_array), np.max(alpha_array), color = \"gray\", linestyle = \"dashed\")\n",
    "        ax[idx, 0].set_yscale(\"log\")\n",
    "        ax[idx, 0].set_ylabel(r\"$\\# \\mathrm{steps}$\")\n",
    "        ax[idx, 0].set_xlabel(r\"$\\alpha$\")\n",
    "\n",
    "        ax[idx, 1].set_xscale(\"log\")\n",
    "        ax[idx, 1].set_ylabel(r\"$\\%_{\\mathrm{model}}$\")\n",
    "        ax[idx, 1].set_xlabel(r\"$\\# \\mathrm{steps}$\")\n",
    "        ax[idx, 1].hlines(100, 1, max_steps, color = \"gray\", linestyle = \"dashed\")\n",
    "\n",
    "        title_sub = \"WalkSAT\" if idx == 1 else \"MT\"\n",
    "        ax[idx,0].text(0.1, 0.90, title_sub, transform=ax[idx,0].transAxes, verticalalignment='top', fontsize=18)\n",
    "        ax[idx,1].text(0.1, 0.90, title_sub, transform=ax[idx,1].transAxes, verticalalignment='top', fontsize=18)\n",
    "\n",
    "    ax_legend = fig.add_axes([0.1, -0.12, 0.8, 0.1])\n",
    "    ax_legend.axis('off')\n",
    "    handles, labels = ax[0, 1].get_legend_handles_labels()\n",
    "    min_line = mlines.Line2D([], [], color=\"gray\", linewidth=1, alpha=1, linestyle='dotted', label=\"using minimum steps across runs\")\n",
    "    median_line = mlines.Line2D([], [], color=\"gray\", linewidth=1.5, alpha=1, linestyle='solid', label='using median steps across runs')\n",
    "    max_line = mlines.Line2D([], [], color=\"gray\", linewidth=1, alpha=1, linestyle='dashed', label='using maximum steps across runs')\n",
    "    if min_max == True:\n",
    "        handles.extend([min_line, median_line, max_line])\n",
    "        labels.extend([min_line.get_label(), median_line.get_label(), max_line.get_label()])\n",
    "    else:\n",
    "        handles.extend([median_line])\n",
    "        labels.extend([median_line.get_label()])\n",
    "    ax_legend.legend(handles, labels, loc='center', ncol = 2)\n",
    "    plt.subplots_adjust(bottom=0.7)\n",
    "    \n",
    "    fig.tight_layout()\n",
    "    if plot_save:\n",
    "        plt.savefig(plot_save + \".pdf\", dpi = 500, format = \"pdf\", bbox_inches='tight')\n",
    "        print(\"saved\")\n",
    "    plt.show()\n",
    "\n",
    "def get_statistics(model_paths, names):\n",
    "        \"\"\"\n",
    "        model_paths: list of paths to the models (i.e. trajectories.npy files from above)\n",
    "        names: list of names of the models (i.e. the names of the models in the same order as the model_paths that appear in the figure legend)\n",
    "        \"\"\"\n",
    "        df = pd.DataFrame(columns=[\"Name\", \"Mean\", \"Median\", \"Percentage Median\", \"Percentage best\", \"Percentage worst\"])\n",
    "        for i in range(len(model_paths)):\n",
    "            if os.path.isfile(model_paths[i]):\n",
    "                array = np.load(model_paths[i], allow_pickle=True)\n",
    "                alpha_list = array[3]\n",
    "                max_steps_algo = len(array[5]) - 1\n",
    "                total_steps = array[6] + np.ones(array[6].shape)\n",
    "                median_steps = np.median(np.median(total_steps,axis = 1))\n",
    "                mean_steps = np.mean(np.mean(total_steps,axis = 1))\n",
    "                median_instances = np.median(total_steps,axis = 1)\n",
    "                percentage_median = np.sum(np.where(np.median(total_steps,axis = 1) < max_steps_algo,1,0)) / len(np.median(total_steps,axis = 1)) * 100\n",
    "                percentage_min = np.sum(np.where(np.amin(total_steps,axis = 1) < max_steps_algo,1,0)) / len(np.mean(total_steps,axis = 1)) * 100\n",
    "                percentage_max = np.sum(np.where(np.amax(total_steps,axis = 1) < max_steps_algo,1,0)) / len(np.mean(total_steps,axis = 1)) * 100\n",
    "                df.loc[i] = [names[i], mean_steps, median_steps, percentage_median, percentage_min, percentage_max]\n",
    "        df[['Mean', 'Median']] = df[['Mean', 'Median']].applymap(\"{:.2e}\".format)\n",
    "        return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.2) Plot the results\n",
    "We compare three different variants of both the MT algorithm and WalkSAT, namely \n",
    "- the original version using the uniform oracle,\n",
    "- a \"hybrid\" version using the trained oracle just for initialization and then switches to uniform updating and\n",
    "- the full oracle-based algorithm using the oracle for both initialization and updating. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "saved_eval_path_list_MT=  [\n",
    "                        \"../../Data/trajectories/random_3SAT/3SATmoseruniform.npy\",\n",
    "                        \"../../Data/trajectories/random_3SAT/3SATmoser_initialize_only3_SAT_Gibbs_LLL.npy\",\n",
    "                        \"../../Data/trajectories/random_3SAT/3SATmoser3_SAT_Gibbs_LLL.npy\", \n",
    "                        ]\n",
    "\n",
    "saved_eval_path_list_WalkSAT =  [\n",
    "                        \"../../Data/trajectories/random_3SAT/3SATprobsatuniform.npy\",\n",
    "                        \"../../Data/trajectories/random_3SAT/3SATprobsat_initialize_only3_SAT_Gibbs_LLL.npy\",\n",
    "                        \"../../Data/trajectories/random_3SAT/3SATprobsat3_SAT_Gibbs_LLL.npy\", \n",
    "                        ]\n",
    "legend = [\"uniform algorithm\", \"``hybrid'' algorithm\", \"full-oracle algorithm\"]\n",
    "\n",
    "color = sns.color_palette(\"Set2\")\n",
    "x = np.linspace(1, 4.4, 71) #specify here the range of alpha values and the spacing you want to average over\n",
    "x_vary = 0.1 #specify here the range of alpha values you want to average over\n",
    "plot_save = \"../../Data/plots/3SAT_main_plot\" #specify here, where you want to save the plot\n",
    "do_evaluation_multiple_models_NEURIPS_legend([saved_eval_path_list_MT, saved_eval_path_list_WalkSAT], color, average_alpha = (x,x_vary), plot_save = plot_save, legend = legend, min_max=True)\n",
    "\n",
    "\n",
    "print(\"MT\")\n",
    "df = get_statistics(saved_eval_path_list_MT, legend)\n",
    "print(df.to_string(index=False))\n",
    "\n",
    "print(\"WalkSAT\")\n",
    "df = get_statistics(saved_eval_path_list_WalkSAT, legend)\n",
    "print(df.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since our loss consists of two terms (that we weigh equally in the experiments), an obvious question to ask is whether both of them contribute to the improvement in the performance of the algorithm. We compare the uniform variant with the full boosted variant, with the model being trained on only one, the other, and both loss terms. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "saved_eval_path_list_MT=  [\n",
    "                        \"../../Data/trajectories/random_3SAT/3SATmoseruniform.npy\",\n",
    "                        \"../../Data/trajectories/random_3SAT/3SATmoser3_SAT_Gibbs_LLL.npy\", \n",
    "                        \"../../Data/trajectories/random_3SAT/3SATmoser3_SAT_Gibbs.npy\",\n",
    "                        \"../../Data/trajectories/random_3SAT/3SATmoser3_SAT_LLL.npy\",\n",
    "                        ]\n",
    "saved_eval_path_list_WalkSAT=  [\n",
    "                        \"../../Data/trajectories/random_3SAT/3SATprobsatuniform.npy\",\n",
    "                        \"../../Data/trajectories/random_3SAT/3SATprobsat3_SAT_Gibbs_LLL.npy\", \n",
    "                        \"../../Data/trajectories/random_3SAT/3SATprobsat3_SAT_Gibbs.npy\",\n",
    "                        \"../../Data/trajectories/random_3SAT/3SATprobsat3_SAT_LLL.npy\",\n",
    "                        ]\n",
    "legend = [\"uniform oracle\" , \"oracle trained using Gibbs- + LLL-Loss\", \"oracle trained using only Gibbs-Loss\", \"oracle trained using only LLL-Loss\"]\n",
    "\n",
    "color = sns.color_palette(\"Set2\")\n",
    "x = np.linspace(1, 4.4, 71) #specify here the range of alpha values and the spacing you want to average over\n",
    "x_vary = 0.1 #specify here the range of alpha values you want to average over\n",
    "plot_save = \"../../Data/plots/3SAT_different_loss_terms\" #specify here, where you want to save the plot\n",
    "do_evaluation_multiple_models_NEURIPS_legend([saved_eval_path_list_MT, saved_eval_path_list_WalkSAT], color, average_alpha = (x,x_vary), plot_save = plot_save, legend = legend, min_max = True)\n",
    "\n",
    "print(\"MT\")\n",
    "df = get_statistics(saved_eval_path_list_MT, legend)\n",
    "print(df.to_string(index=False))\n",
    "\n",
    "print(\"WalkSAT\")\n",
    "df = get_statistics(saved_eval_path_list_WalkSAT, legend)\n",
    "print(df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.) Experiments on pseudo-industrial datasets\n",
    "\n",
    "### 2.1) Load a model and run the WalkSAT on it\n",
    "As above, we start by running WalkSAT with the trained models on the datasets. The code below also does all the cross-evaluations. If you want to avoid this, please change the code accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from evaluate_with_given_params import load_model_and_test\n",
    "\n",
    "# create the folders to save the trajectories\n",
    "mkdir! ../../Data/trajectories/g4sat_easy/ca/\n",
    "mkdir! ../../Data/trajectories/g4sat_easy/ps/\n",
    "mkdir! ../../Data/trajectories/g4sat_medium/ca/\n",
    "mkdir! ../../Data/trajectories/g4sat_medium/ps/\n",
    "mkdir! ../../Data/trajectories/g4sat_hard/ca/\n",
    "mkdir! ../../Data/trajectories/g4sat_hard/ps/\n",
    "\n",
    "data_base_path = \"../../Data/G4SAT/\" # fill this with the path to the G4SAT datasets\n",
    "traj_data = [\n",
    "            [\"../../Data/trajectories/g4sat_easy/ca/\", data_base_path + \"easy/ca/test/sat/\"],\n",
    "            [\"../../Data/trajectories/g4sat_easy/ps/\", data_base_path + \"easy/ps/test/sat/\"],\n",
    "            [\"../../Data/trajectories/g4sat_medium/ca/\", data_base_path + \"medium/ca/test/sat/\"],\n",
    "            [\"../../Data/trajectories/g4sat_medium/ps/\", data_base_path + \"medium/ps/test/sat/\"],\n",
    "            [\"../../Data/trajectories/g4sat_hard/ca/\", data_base_path + \"hard/ca/test/sat/\"],\n",
    "            [\"../../Data/trajectories/g4sat_hard/ps/\", data_base_path + \"hard/ps/test/sat/\"],\n",
    "            ]\n",
    "#put the paths of the models you want to evaluate\n",
    "model_paths = [ \n",
    "                \"uniform\",\n",
    "                \"../../Data/models/pseudo_industrial/g4sat_easy_ca.npy\",\n",
    "                \"../../Data/models/pseudo_industrial/g4sat_easy_ps.npy\",\n",
    "                \"../../Data/models/pseudo_industrial/g4sat_medium_ca.npy\", \n",
    "                \"../../Data/models/pseudo_industrial/g4sat_medium_ps.npy\",\n",
    "                \"../../Data/models/pseudo_industrial/g4sat_hard_ca.npy\",\n",
    "                \"../../Data/models/pseudo_industrial/g4sat_hard_ps.npy\",\n",
    "                ]\n",
    "# in case you train your own models, please adjust the filename accordingly.\n",
    "\n",
    "n_steps = 10000\n",
    "n_runs = 100\n",
    "\n",
    "for i in range(len(traj_data)):\n",
    "    data_path = traj_data[i][1]\n",
    "    trajectories_path = traj_data[i][0]\n",
    "    for j in range(len(model_paths)):\n",
    "        model_path = model_paths[j]\n",
    "        if model_path != \"uniform\":\n",
    "            path_save = trajectories_path + \"probsat\" + model_path.split(\"/\")[-1][:-4]\n",
    "        else:\n",
    "            path_save = trajectories_path + \"probsat\" + model_path\n",
    "        total_array2 = load_model_and_test(\n",
    "                                    data_path,\n",
    "                                    model_path,\n",
    "                                    n_steps,\n",
    "                                    n_runs,\n",
    "                                    \"probsat\", #moser for MT-algorithm or probsat for oracle WalkSAT\n",
    "                                    path_save=path_save,\n",
    "                                    keep_traj=False,\n",
    "                                    pre_compute_mapping=True,\n",
    "                                    prob_flip_best=0,\n",
    "                                )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2) Plot the trajectories from the evaluation above\n",
    "\n",
    "#### 2.2.1) Define helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "import seaborn as sns\n",
    "import matplotlib.lines as mlines\n",
    "import os\n",
    "\n",
    "# Set global plot parameters\n",
    "plt.rcParams['axes.labelsize'] = 18\n",
    "plt.rcParams['axes.titlesize'] = 18\n",
    "plt.rcParams['legend.fontsize'] = 12\n",
    "plt.rcParams['text.usetex'] = True\n",
    "plt.rcParams[\"font.family\"] = \"Times New Roman\"\n",
    "\n",
    "def get_evaluation_multiple(models, model_paths, names, title, max_steps, path_save = False):\n",
    "\n",
    "    def plot_steps_multiple(idx,min_steps, median_steps, max_steps, max_plot_steps, color, model_label):\n",
    "        lines = []\n",
    "        for steps, linewidth, alpha, linestyle in zip([min_steps, median_steps, max_steps], \n",
    "                                                    [1, 1.5,1], \n",
    "                                                    [1, 1, 1],\n",
    "                                                    ['dotted', 'solid', 'dashed']):\n",
    "            steps = np.array(steps, dtype=int)\n",
    "            bins = np.logspace(0, np.log10(max_plot_steps), num=100)\n",
    "            counts, bins = np.histogram(steps, bins=bins)\n",
    "            \n",
    "            cumulative_counts = np.cumsum(counts)\n",
    "            percentages = cumulative_counts / len(steps) * 100\n",
    "            if linestyle == 'solid':\n",
    "                line, = axs[idx,0].plot(bins[:-1], percentages, color=color, linewidth=linewidth, alpha=alpha, linestyle=linestyle, label=model_label)\n",
    "            else:\n",
    "                line, = axs[idx,0].plot(bins[:-1], percentages, color=color, linewidth=linewidth, alpha=alpha, linestyle=linestyle)\n",
    "            lines.append(line)\n",
    "\n",
    "    def plot_percentage_median_multiple(idx,uniform_steps, model_steps, max_plot_steps, color, model_label):\n",
    "        steps_uniform_median = np.array(np.median(uniform_steps,axis = 1), dtype=int)\n",
    "        steps_model_median = np.array(np.median(model_steps, axis = 1), dtype=int)\n",
    "        bins = np.logspace(0, np.log10(max_plot_steps), num=100)\n",
    "        counts_uniform_median, bins_uniform_median = np.histogram(steps_uniform_median, bins=bins)\n",
    "        counts_model_median, bins_model_median = np.histogram(steps_model_median, bins=bins)\n",
    "        cumulative_counts_uniform_median = np.cumsum(counts_uniform_median)\n",
    "        cumulative_counts_model_median = np.cumsum(counts_model_median)\n",
    "        percentages_uniform_median = cumulative_counts_uniform_median / len(steps_uniform_median) * 100\n",
    "        percentages_model_median = cumulative_counts_model_median / len(steps_uniform_median) * 100\n",
    "        axs[idx,1].plot(bins[:-1], percentages_model_median - percentages_uniform_median, color=color, linewidth=1.5, alpha=1, linestyle=\"solid\", label=model_label)\n",
    "\n",
    "    fig, axs = plt.subplots(len(models), 2, figsize=(8, 3*len(models)))  # Create subplots for each model\n",
    "    for idx, model in enumerate(models):\n",
    "        colors = sns.color_palette(\"Set1\")\n",
    "        uniform_array = np.load(model_paths[idx][len(model_paths[idx])-1], allow_pickle=True)\n",
    "        uniform_steps = uniform_array[6] + np.ones(uniform_array[6].shape)\n",
    "        #max_steps = len(uniform_array[5]) - 1\n",
    "\n",
    "        df = pd.DataFrame(columns=[\"Name\", \"Mean\", \"Median\", \"Percentage Median\", \"Percentage best\", \"Percentage worst\"])\n",
    "        for i in range(len(model_paths[idx])):\n",
    "            if os.path.isfile(model_paths[idx][i]):\n",
    "                array = np.load(model_paths[idx][i], allow_pickle=True)\n",
    "                total_steps = array[6] + np.ones(array[6].shape)\n",
    "                median = np.median(np.median(total_steps,axis = 1))\n",
    "                mean = np.mean(np.mean(total_steps,axis = 1))\n",
    "                percentage_median = np.sum(np.median(total_steps,axis = 1) < max_steps) / len(np.median(total_steps,axis = 1)) * 100\n",
    "                percentage_min = np.sum(np.amin(total_steps, axis = 1) < max_steps) / len(np.mean(total_steps,axis = 1)) * 100\n",
    "                percentage_max = np.sum(np.amax(total_steps, axis = 1) < max_steps) / len(np.mean(total_steps,axis = 1)) * 100\n",
    "                df.loc[i] = [names[i], mean, median, percentage_median, percentage_min, percentage_max]    \n",
    "                plot_steps_multiple(idx, np.amin(total_steps, axis = 1), np.median(total_steps, axis = 1), np.amax(total_steps, axis = 1), max_steps - 1, colors[i], names[i])\n",
    "            else:\n",
    "                print(\"Model not found\")\n",
    "        axs[idx,0].hlines(100,0, max_steps - 1, colors=\"black\", linestyles=\"dashed\")\n",
    "        axs[idx,0].set_xlabel(r\"$\\# \\mathrm{steps}$\")\n",
    "        axs[idx,0].set_ylabel(r\"$\\%_{\\mathrm{model}}$\")\n",
    "        axs[idx,0].set_xscale(\"log\")\n",
    "        if idx == 0:\n",
    "            title_sub = \"easy\"\n",
    "        elif idx == 1:\n",
    "            title_sub = \"medium\"\n",
    "        else:\n",
    "            title_sub = \"hard\"\n",
    "        axs[idx,0].text(0.1, 0.90, title_sub, transform=axs[idx,0].transAxes, verticalalignment='top', fontsize = 18)\n",
    "\n",
    "        for i in range(len(model_paths[idx])):\n",
    "            if os.path.isfile(model_paths[idx][i]):\n",
    "                array = np.load(model_paths[idx][i], allow_pickle=True)\n",
    "                total_steps = array[6] + np.ones(array[6].shape)\n",
    "                plot_percentage_median_multiple(idx, uniform_steps, total_steps, max_steps -1, colors[i], names[i])\n",
    "            else:\n",
    "                print(\"Model not found\")\n",
    "        axs[idx,1].set_ylabel(r\"$\\%_{\\mathrm{model}} - \\%_{\\mathrm{uniform}}$\")\n",
    "        axs[idx,1].set_xlabel(r\"$\\# \\mathrm{steps}$\")\n",
    "        axs[idx,1].set_xscale(\"log\")\n",
    "        axs[idx,1].text(0.1, 0.90, title_sub, transform=axs[idx,1].transAxes, verticalalignment='top', fontsize = 18)\n",
    "        df[['Mean', 'Median']] = df[['Mean', 'Median']].applymap(\"{:.2e}\".format)\n",
    "        df_string = df.to_string(index=False)\n",
    "        # df_string = df.to_latex(index=False)\n",
    "        # print(\"/\".join(model.split(\"/\")[-3:]))\n",
    "        print(df_string)\n",
    "\n",
    "    ax_legend = fig.add_axes([0.1, -0.12, 0.8, 0.1])\n",
    "    ax_legend.axis('off')\n",
    "    handles, labels = axs[0, 0].get_legend_handles_labels()\n",
    "    min_line = mlines.Line2D([], [], color=\"gray\", linewidth=1, alpha=1, linestyle='dotted', label=\"using minimum steps across runs\")\n",
    "    median_line = mlines.Line2D([], [], color=\"gray\", linewidth=1.5, alpha=1, linestyle='solid', label='using median steps across runs')\n",
    "    max_line = mlines.Line2D([], [], color=\"gray\", linewidth=1, alpha=1, linestyle='dashed', label='using maximum steps across runs')\n",
    "    handles.extend([min_line, median_line, max_line])\n",
    "    labels.extend([min_line.get_label(), median_line.get_label(), max_line.get_label()])\n",
    "    ax_legend.legend(handles, labels, loc='center', ncol = 2)\n",
    "    plt.subplots_adjust(bottom=0.7)\n",
    "    \n",
    "    fig.suptitle(title, fontsize=20)\n",
    "    fig.tight_layout()\n",
    "    if path_save:\n",
    "        plt.savefig(path_save + \".pdf\", dpi = 500, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "def get_statistics_relative_improvement(model_paths, names, max_steps_algo):\n",
    "        \"\"\"\n",
    "        model_paths: list of paths to the models (i.e. trajectories.npy files from above)\n",
    "        names: list of names of the models (i.e. the names of the models in the same order as the model_paths that appear in the figure legend)\n",
    "        \"\"\"\n",
    "        df = pd.DataFrame(columns=[\"Name\", \"Mean\", \"Median\", \"Percentage Median\", \"Percentage best\", \"Percentage worst\"])\n",
    "        error = 0\n",
    "        for i in range(len(model_paths)):\n",
    "            if os.path.isfile(model_paths[i]):\n",
    "                array = np.load(model_paths[i], allow_pickle=True)\n",
    "                alpha_list = array[3]\n",
    "                # max_steps_algo = len(array[5]) - 1\n",
    "                total_steps = array[6] + np.ones(array[6].shape)\n",
    "                median_steps = np.median(np.median(total_steps,axis = 1))\n",
    "                mean_steps = np.mean(np.mean(total_steps,axis = 1))\n",
    "                median_instances = np.median(total_steps,axis = 1)\n",
    "                percentage_median = np.sum(np.where(np.median(total_steps,axis = 1) < max_steps_algo,1,0)) / len(np.median(total_steps,axis = 1)) * 100\n",
    "                percentage_min = np.sum(np.where(np.amin(total_steps,axis = 1) < max_steps_algo,1,0)) / len(np.mean(total_steps,axis = 1)) * 100\n",
    "                percentage_max = np.sum(np.where(np.amax(total_steps,axis = 1) < max_steps_algo,1,0)) / len(np.mean(total_steps,axis = 1)) * 100\n",
    "                df.loc[i] = [names[i], mean_steps, median_steps, percentage_median, percentage_min, percentage_max]\n",
    "            else: \n",
    "                error = 1\n",
    "        if error == 0:\n",
    "            df.loc[2] = [\"relative_improvement\", ((df.loc[0][\"Mean\"] - df.loc[1][\"Mean\"])/ df.loc[1][\"Mean\"]) * 100, ((df.loc[0][\"Median\"] - df.loc[1][\"Median\"])/ df.loc[1][\"Median\"]) * 100, ((df.loc[0][\"Percentage Median\"] - df.loc[1][\"Percentage Median\"])/ df.loc[1][\"Percentage Median\"]) * 100, ((df.loc[0][\"Percentage best\"] - df.loc[1][\"Percentage best\"])/ df.loc[1][\"Percentage best\"]) * 100, ((df.loc[0][\"Percentage worst\"] - df.loc[1][\"Percentage worst\"])/ df.loc[1][\"Percentage worst\"]) * 100]\n",
    "\n",
    "        df[['Mean', 'Median']] = df[['Mean', 'Median']].applymap(\"{:.2e}\".format)\n",
    "        return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.2) Plot the results\n",
    "\n",
    "We plot the cross-benchmark results and the corresponding statistics on the CA datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = [\n",
    "        \"WalkSAT with oracle trained on CA (easy)\",\n",
    "        \"WalkSAT with oracle trained on CA (medium)\",\n",
    "        \"WalkSAT with oracle trained on CA (hard)\",\n",
    "        \"WalkSAT with oracle trained on PS (easy)\",\n",
    "        \"WalkSAT with oracle trained on PS (medium)\",\n",
    "        \"WalkSAT with oracle trained on PS (hard)\",\n",
    "        \"uniform WalkSAT\"\n",
    "        ]\n",
    "\n",
    "traj_names = [\n",
    "        \"probsatg4sat_easy_ca.npy\",\n",
    "        \"probsatg4sat_medium_ca.npy\",\n",
    "        \"probsatg4sat_hard_ca.npy\",\n",
    "        \"probsatg4sat_easy_ps.npy\",\n",
    "        \"probsatg4sat_medium_ps.npy\",\n",
    "        \"probsatg4sat_hard_ps.npy\",\n",
    "        \"probsatuniform.npy\",\n",
    "]    \n",
    "\n",
    "models = [\n",
    "        \"../../Data/trajectories/g4sat_easy/ca/\",\n",
    "        \"../../Data/trajectories/g4sat_medium/ca/\",\n",
    "        \"../../Data/trajectories/g4sat_hard/ca/\",\n",
    "]\n",
    "\n",
    "model_paths = []\n",
    "for model in models:\n",
    "    model_paths.append([model + traj_name for traj_name in traj_names])\n",
    "plot_save = \"../../Data/plots/pseudo_industrial_CA_cross_benchmark\"\n",
    "title = \"CA datasets\"\n",
    "get_evaluation_multiple(models, model_paths, names, title, path_save = plot_save, max_steps = 10**6 - 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and the same on the PS datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = [\n",
    "        \"WalkSAT with oracle trained on CA (easy)\",\n",
    "        \"WalkSAT with oracle trained on CA (medium)\",\n",
    "        \"WalkSAT with oracle trained on CA (hard)\",\n",
    "        \"WalkSAT with oracle trained on PS (easy)\",\n",
    "        \"WalkSAT with oracle trained on PS (medium)\",\n",
    "        \"WalkSAT with oracle trained on PS (hard)\",\n",
    "        \"uniform WalkSAT\"\n",
    "        ]\n",
    "\n",
    "traj_names = [\n",
    "        \"probsatg4sat_easy_ca.npy\",\n",
    "        \"probsatg4sat_medium_ca.npy\",\n",
    "        \"probsatg4sat_hard_ca.npy\",\n",
    "        \"probsatg4sat_easy_ps.npy\",\n",
    "        \"probsatg4sat_medium_ps.npy\",\n",
    "        \"probsatg4sat_hard_ps.npy\",\n",
    "        \"probsatuniform.npy\",\n",
    "]    \n",
    "\n",
    "models = [\n",
    "        \"../../Data/trajectories/g4sat_easy/ps/\",\n",
    "        \"../../Data/trajectories/g4sat_medium/ps/\",\n",
    "        \"../../Data/trajectories/g4sat_hard/ps/\",\n",
    "]\n",
    "\n",
    "model_paths = []\n",
    "for model in models:\n",
    "    model_paths.append([model + traj_name for traj_name in traj_names])\n",
    "plot_save = \"../../Data/plots/pseudo_industrial_PS_cross_benchmark\"\n",
    "title = \"PS datasets\"\n",
    "get_evaluation_multiple(models, model_paths, names, title, max_steps = 10**6 - 1, path_save = plot_save)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are only interested in the statistics of two models and you want to see the improvement over each other, you can get them in the following way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traj_paths = [\n",
    "        \"probsatg4sat_easy_ca.npy\",\n",
    "        \"probsatg4sat_medium_ca.npy\",\n",
    "        \"probsatg4sat_hard_ca.npy\",\n",
    "        \"probsatg4sat_easy_ps.npy\",\n",
    "        \"probsatg4sat_medium_ps.npy\",\n",
    "        \"probsatg4sat_hard_ps.npy\",\n",
    "        \"probsatuniform.npy\",\n",
    "]    \n",
    "\n",
    "models = [\n",
    "        \"../../Data/trajectories/g4sat_easy/ca/\",\n",
    "        \"../../Data/trajectories/g4sat_medium/ca/\",\n",
    "        \"../../Data/trajectories/g4sat_hard/ca/\",\n",
    "        \"../../Data/trajectories/g4sat_easy/ps/\",\n",
    "        \"../../Data/trajectories/g4sat_medium/ps/\",\n",
    "        \"../../Data/trajectories/g4sat_hard/ps/\",\n",
    "]\n",
    "\n",
    "for i in range(len(models)):\n",
    "    print(models[i])\n",
    "    saved_eval_path_list = [ \n",
    "        models[i] + traj_paths[i],\n",
    "        models[i] + \"probsatuniform.npy\",\n",
    "    ]\n",
    "\n",
    "    legend = [\n",
    "        traj_paths[i],\n",
    "        \"uniform WalkSAT\"]\n",
    "\n",
    "    df = get_statistics_relative_improvement(saved_eval_path_list, legend, max_steps_algo=10**6 - 1)\n",
    "    print(df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
